{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch torchvision torchaudio gpt4all tiktoken blobfile protobuf sentencepiece accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"lmsys/vicuna-7b-v1.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
    "with model.chat_session():\n",
    "    print(model.generate(\"<|system|>You are an AI assistant that provides short, conversational responses.<|end|><|user|>How was your day today?<|end|><|assistant|>\", max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# offload_folder = \"./offload_weights\"\n",
    "# # Load model and tokenizer\n",
    "# model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=\"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\", offload_folder=offload_folder, torch_dtype=\"float16\")\n",
    "\n",
    "# plain_text_prompt = \"\"\"System: You are an AI assistant that provides short, conversational responses.\n",
    "# User: How was your day today?\n",
    "# Assistant:\"\"\"\n",
    "\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "\n",
    "# # Tokenize the prompt\n",
    "# input_ids = tokenizer(plain_text_prompt, return_tensors=\"pt\").input_ids\n",
    "# input_ids = input_ids.to(\"mps\")\n",
    "\n",
    "# # Stream response for real-time output\n",
    "# streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# # Generate response\n",
    "# output_ids = model.generate(input_ids, max_new_tokens=50, streamer=streamer)\n",
    "\n",
    "# # Decode and print response\n",
    "# response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "# Load the model\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")  # Make sure the model file exists\n",
    "\n",
    "# Define system prompt\n",
    "# system_prompt = \"<|system|>You are an AI assistant that provides short, conversational response options for AAC users. Here are the options your user has selected so far, if any conversation is about these options use the context from these options!: \"\n",
    "system_prompt = \"<|system|>You are an AI assistant that provides short, conversational response options for AAC users. Here is the conversation so far, use it to guide your response: \"\n",
    "# Start interactive chat session\n",
    "with model.chat_session():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")  # Get user input\n",
    "        \n",
    "        if user_input.lower() == \"exit\":  # Exit condition\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Construct prompt to get short response options\n",
    "        prompt = f\"{system_prompt}<|end|><|user|>{user_input}<|end|><|assistant|>Provide 4 short response options to choose from:<|end|>\"\n",
    "\n",
    "        # Generate response\n",
    "        response = model.generate(prompt, max_tokens=100)\n",
    "\n",
    "        # Print response options\n",
    "        print(\"\\nAI Response Options:\")\n",
    "        options = response.strip().split(\"\\n\")  # Split responses by newline if formatted that way\n",
    "        for i, option in enumerate(options, start=1):\n",
    "            print(f\"{i}. {option.strip()}\")\n",
    "            \n",
    "        # Get user's choice\n",
    "        choice = input(\"\\nSelect a response option: \")\n",
    "        # if choice not in [\"1\", \"2\", \"3\"]:\n",
    "        #     print(\"Invalid choice. Please select 1, 2, or 3.\")\n",
    "\n",
    "        system_prompt = system_prompt + \" \" + \"They said: \" + user_input + \" \" + \" you responded: \" + options[int(choice) - 1]\n",
    "        print(system_prompt)\n",
    "        print(\"\\n(Type 'exit' to quit)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Initialize models\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Vector store setup\n",
    "INDEX_FILE = \"rag_index.faiss\"\n",
    "DOCS_FILE = \"rag_docs.pkl\"\n",
    "dimension = 384\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "documents = []\n",
    "\n",
    "if os.path.exists(INDEX_FILE) and os.path.exists(DOCS_FILE):\n",
    "    index = faiss.read_index(INDEX_FILE)\n",
    "    with open(DOCS_FILE, \"rb\") as f:\n",
    "        documents = pickle.load(f)\n",
    "\n",
    "def add_to_knowledge(text):\n",
    "    vec = embed_model.encode([text])\n",
    "    index.add(vec)\n",
    "    documents.append(text)\n",
    "    faiss.write_index(index, INDEX_FILE)\n",
    "    with open(DOCS_FILE, \"wb\") as f:\n",
    "        pickle.dump(documents, f)\n",
    "\n",
    "def retrieve_context(query, k=3):\n",
    "    vec = embed_model.encode([query])\n",
    "    _, I = index.search(vec, k)\n",
    "    return [documents[i] for i in I[0]]\n",
    "\n",
    "# Interactive session with RAG\n",
    "with model.chat_session():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Retrieve relevant history\n",
    "        relevant_context = retrieve_context(user_input)\n",
    "        context = \"\\n\".join(relevant_context)\n",
    "\n",
    "        # Generate response\n",
    "        prompt = f\"<|system|>You are an AI assistant. Hereâ€™s some past conversation for context:\\n{context}\\n<|end|><|user|>{user_input}<|end|><|assistant|>\"\n",
    "\n",
    "        response = model.generate(prompt, max_tokens=300)\n",
    "        print(\"Assistant:\", response)\n",
    "\n",
    "        # Add user input to RAG memory\n",
    "        add_to_knowledge(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atharva/Documents/CSE/NLP/final/ve/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Here are 10 short response options for an AAC user:\n",
      "2. \n",
      "3. 1. Good! Thanks!\n",
      "4. 2. Passed with flying colors!\n",
      "5. 3. Okay, it's over now.\n",
      "6. 4. I did my best.\n",
      "7. 5. Not bad, thanks!\n",
      "8. 6. Still waiting to hear...\n",
      "9. 7. Fingers crossed!\n",
      "10. 8. It went well, thank you!\n",
      "11. 9. Trying not to think about it\n",
      "12. 10. Relieved it's done!\n",
      "13. \n",
      "14. These responses are designed to be short and easy to understand for AAC users who may have limited communication abilities or need assistance with expressing themselves.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, option \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(options, start=\u001b[32m1\u001b[39m):\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m you = \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mselect a response option\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mYou selected?\u001b[39m\u001b[33m\"\u001b[39m, you, options[you])\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Store the user message to context store\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: invalid literal for int() with base 10: ''"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "from rag_store import add_context, retrieve_context\n",
    "\n",
    "# Load local LLaMA model\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")\n",
    "\n",
    "# Start interactive chat\n",
    "with model.chat_session():\n",
    "    while True:\n",
    "        user_input = input(\"Person: \")\n",
    "\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Get context from RAG\n",
    "        relevant_context = retrieve_context(user_input)\n",
    "        context_block = \"\\n\".join(relevant_context)\n",
    "\n",
    "        # Build prompt\n",
    "        prompt = f\"<|system|>You are an AI assistant that provides short, conversational response options for AAC users. Use the following conversation context:\\n{context_block}\\n<|end|><|user|>{user_input}<|end|><|assistant|>Provide 10 short, helpful responses:\\n\"\n",
    "\n",
    "        # Get response\n",
    "        response = model.generate(prompt, max_tokens=200)\n",
    "        # print(\"Assistant:\", response.strip())\n",
    "        options = response.strip().split(\"\\n\")\n",
    "        for i, option in enumerate(options, start=1):\n",
    "            print(f\"{i}. {option.strip()}\")\n",
    "        you = int(input(\"select a response option\"))\n",
    "        print(\"You selected?\", you, options[you])\n",
    "\n",
    "        # Store the user message to context store\n",
    "        add_context(user_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
