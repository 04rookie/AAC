{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers torch torchvision torchaudio gpt4all tiktoken blobfile protobuf sentencepiece accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use a pipeline as a high-level helper\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\"text-generation\", model=\"lmsys/vicuna-7b-v1.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gpt4all import GPT4All\n",
    "# model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
    "# with model.chat_session():\n",
    "#     print(model.generate(\"<|system|>I will give you a sentence and a word after |, I want you to generate 3 wrong words in place of that word that are wrong.<|end|>\", max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "# offload_folder = \"./offload_weights\"\n",
    "# # Load model and tokenizer\n",
    "# model_name = \"Open-Orca/Mistral-7B-OpenOrca\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, token=\"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\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", token=\"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\", offload_folder=offload_folder, torch_dtype=\"float16\")\n",
    "\n",
    "# plain_text_prompt = \"\"\"System: You are an AI assistant that provides short, conversational responses.\n",
    "# User: How was your day today?\n",
    "# Assistant:\"\"\"\n",
    "\n",
    "\n",
    "# from transformers import TextStreamer\n",
    "\n",
    "# # Tokenize the prompt\n",
    "# input_ids = tokenizer(plain_text_prompt, return_tensors=\"pt\").input_ids\n",
    "# input_ids = input_ids.to(\"mps\")\n",
    "\n",
    "# # Stream response for real-time output\n",
    "# streamer = TextStreamer(tokenizer)\n",
    "\n",
    "# # Generate response\n",
    "# output_ids = model.generate(input_ids, max_new_tokens=50, streamer=streamer)\n",
    "\n",
    "# # Decode and print response\n",
    "# response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "# print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Response Options:\n",
      "1. Here are three incorrect words:\n",
      "2. \n",
      "3. 1. pineapple\n",
      "4. 2. astronaut\n",
      "5. 3. accordion\n",
      "<|system|>I will give you a sentence and a word after |, I want you to generate 3 wrong words in place of that word that are wrong.<|end|>\n",
      "\n",
      "(Type 'exit' to quit)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "# Load the model\n",
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\")  # Make sure the model file exists\n",
    "\n",
    "# Define system prompt\n",
    "# system_prompt = \"<|system|>You are an AI assistant that provides short, conversational response options for AAC users. Here are the options your user has selected so far, if any conversation is about these options use the context from these options!: \"\n",
    "system_prompt = \"<|system|>I will give you a sentence and a word after |, I want you to generate 3 wrong words in place of that word that are wrong.<|end|>\"\n",
    "# Start interactive chat session\n",
    "with model.chat_session():\n",
    "    while True:\n",
    "        user_input = input(\"You: \")  # Get user input\n",
    "        \n",
    "        if user_input.lower() == \"exit\":  # Exit condition\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Construct prompt to get short response options\n",
    "        prompt = f\"{system_prompt}<|user|>{user_input}<|end|><|assistant|>Give me 3 wrong words, they should fit the context but wrong:<|end|>\"\n",
    "\n",
    "        # Generate response\n",
    "        response = model.generate(prompt, max_tokens=40)\n",
    "\n",
    "        # Print response options\n",
    "        print(\"\\nAI Response Options:\")\n",
    "        options = response.strip().split(\"\\n\")  # Split responses by newline if formatted that way\n",
    "        for i, option in enumerate(options, start=1):\n",
    "            print(f\"{i}. {option.strip()}\")\n",
    "            \n",
    "        # Get user's choice\n",
    "        # choice = input(\"\\nSelect a response option: \")\n",
    "        # if choice not in [\"1\", \"2\", \"3\"]:\n",
    "        #     print(\"Invalid choice. Please select 1, 2, or 3.\")\n",
    "\n",
    "        # system_prompt = system_prompt + \" \" + \"They said: \" + user_input + \" \" + \" you responded: \" + options[int(choice) - 1]\n",
    "        print(system_prompt)\n",
    "        print(\"\\n(Type 'exit' to quit)\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
